{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFyatu9WlCsm"
      },
      "outputs": [],
      "source": [
        "# Installer la bibliothèque Kaggle\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Téléverser votre fichier kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "#!mv \"kaggle (1).json\" kaggle.json\n",
        "\n",
        "# Configurer Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Télécharger le dataset \"Road Sign Detection\"\n",
        "!kaggle datasets download -d andrewmvd/road-sign-detection\n",
        "\n",
        "# Décompresser le fichier téléchargé\n",
        "!mkdir road_sign_detection\n",
        "!unzip road-sign-detection.zip -d road_sign_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqKc9vtglM34"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio transformers pycocotools opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND45lyTylNI2"
      },
      "outputs": [],
      "source": [
        "!pip install -q ultralytics tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TNVy1ifclTxo"
      },
      "outputs": [],
      "source": [
        "import torch as torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import albumentations as A\n",
        "import torchaudio\n",
        "from pycocotools.coco import COCO\n",
        "import cv2\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, T5TokenizerFast, T5ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L5OTzbFTzGOm"
      },
      "outputs": [],
      "source": [
        "class_names = ['speedlimit', 'stop', 'crosswalk', 'trafficlight']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9ySzo4ylfel"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ultralytics import YOLO\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Fonction de conversion des annotations de Pascal VOC à YOLO\n",
        "def convert_voc_to_yolo(voc_dir, yolo_dir, class_names):\n",
        "    if not os.path.exists(yolo_dir):\n",
        "        os.makedirs(yolo_dir)\n",
        "\n",
        "    for xml_file in os.listdir(voc_dir):\n",
        "        if not xml_file.endswith('.xml'):\n",
        "            continue\n",
        "\n",
        "        tree = ET.parse(os.path.join(voc_dir, xml_file))\n",
        "        root = tree.getroot()\n",
        "\n",
        "        image_size = root.find('size')\n",
        "        image_width = int(image_size.find('width').text)\n",
        "        image_height = int(image_size.find('height').text)\n",
        "\n",
        "        yolo_annotation = []\n",
        "        for obj in root.findall('object'):\n",
        "            class_name = obj.find('name').text\n",
        "            if class_name not in class_names:\n",
        "                continue\n",
        "\n",
        "            class_id = class_names.index(class_name)\n",
        "\n",
        "            bndbox = obj.find('bndbox')\n",
        "            xmin = int(bndbox.find('xmin').text)\n",
        "            ymin = int(bndbox.find('ymin').text)\n",
        "            xmax = int(bndbox.find('xmax').text)\n",
        "            ymax = int(bndbox.find('ymax').text)\n",
        "\n",
        "            x_center = (xmin + xmax) / 2.0 / image_width\n",
        "            y_center = (ymin + ymax) / 2.0 / image_height\n",
        "            width = (xmax - xmin) / image_width\n",
        "            height = (ymax - ymin) / image_height\n",
        "\n",
        "            yolo_annotation.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
        "\n",
        "        yolo_file = os.path.join(yolo_dir, xml_file.replace('.xml', '.txt'))\n",
        "        with open(yolo_file, 'w') as f:\n",
        "            f.write('\\n'.join(yolo_annotation))\n",
        "\n",
        "# Chemins vers les dossiers\n",
        "root_dir = '/content/road_sign_detection'\n",
        "images_dir = os.path.join(root_dir, 'images')\n",
        "annotations_dir = os.path.join(root_dir, 'annotations')\n",
        "\n",
        "train_dir = os.path.join(root_dir, 'train')\n",
        "val_dir = os.path.join(root_dir, 'val')\n",
        "test_dir = os.path.join(root_dir, 'test')\n",
        "\n",
        "# Créer les répertoires de sortie\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'images'), exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'annotations'), exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'images'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'annotations'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(test_dir, 'images'), exist_ok=True)\n",
        "os.makedirs(os.path.join(test_dir, 'annotations'), exist_ok=True)\n",
        "os.makedirs(os.path.join(test_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "# Obtenir les chemins des fichiers et trier\n",
        "image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.png') or f.endswith('.jpg')])\n",
        "annotation_files = sorted([f for f in os.listdir(annotations_dir) if f.endswith('.xml')])\n",
        "\n",
        "# Vérification des fichiers trouvés\n",
        "print(\"Images trouvées:\", image_files)\n",
        "print(\"Annotations trouvées:\", annotation_files)\n",
        "\n",
        "# Diviser les données en train (80%), val (10%) et test (10%)\n",
        "train_images, test_images, train_annotations, test_annotations = train_test_split(\n",
        "    image_files, annotation_files, test_size=0.2, random_state=42, shuffle=False\n",
        ")\n",
        "\n",
        "val_images, test_images, val_annotations, test_annotations = train_test_split(\n",
        "    test_images, test_annotations, test_size=0.5, random_state=42, shuffle=False\n",
        ")\n",
        "\n",
        "def move_files(file_list, source_dir, dest_dir):\n",
        "    for file_name in file_list:\n",
        "        shutil.move(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))\n",
        "\n",
        "# Déplacer les fichiers\n",
        "move_files(train_images, images_dir, os.path.join(train_dir, 'images'))\n",
        "move_files(train_annotations, annotations_dir, os.path.join(train_dir, 'annotations'))\n",
        "\n",
        "move_files(val_images, images_dir, os.path.join(val_dir, 'images'))\n",
        "move_files(val_annotations, annotations_dir, os.path.join(val_dir, 'annotations'))\n",
        "\n",
        "move_files(test_images, images_dir, os.path.join(test_dir, 'images'))\n",
        "move_files(test_annotations, annotations_dir, os.path.join(test_dir, 'annotations'))\n",
        "\n",
        "# Convertir les annotations pour les ensembles d'entraînement, de validation et de test\n",
        "convert_voc_to_yolo('/content/road_sign_detection/train/annotations', '/content/road_sign_detection/train/labels', class_names)\n",
        "convert_voc_to_yolo('/content/road_sign_detection/val/annotations', '/content/road_sign_detection/val/labels', class_names)\n",
        "convert_voc_to_yolo('/content/road_sign_detection/test/annotations', '/content/road_sign_detection/test/labels', class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Définir les classes\n",
        "class_names = ['speedlimit', 'stop', 'crosswalk', 'trafficlight']\n",
        "\n",
        "# Initialiser le dictionnaire des occurrences\n",
        "class_counts = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "# Dossiers des annotations\n",
        "annotations_dirs = [\n",
        "    '/content/road_sign_detection/train/annotations',\n",
        "    '/content/road_sign_detection/val/annotations',\n",
        "    '/content/road_sign_detection/test/annotations'\n",
        "]\n",
        "\n",
        "# Parcourir tous les dossiers d'annotations\n",
        "for annotations_dir in annotations_dirs:\n",
        "    # Parcourir tous les fichiers d'annotations dans le dossier\n",
        "    for filename in os.listdir(annotations_dir):\n",
        "        if filename.endswith('.xml'):\n",
        "            file_path = os.path.join(annotations_dir, filename)\n",
        "            tree = ET.parse(file_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            # Parcourir toutes les balises 'object' pour trouver les étiquettes\n",
        "            for obj in root.findall('object'):\n",
        "                label = obj.find('name').text\n",
        "                if label in class_counts:\n",
        "                    class_counts[label] += 1\n",
        "\n",
        "# Afficher le dictionnaire des occurrences\n",
        "print(class_counts)\n",
        "\n",
        "# Tracer l'histogramme des occurrences\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color='blue')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Occurrences')\n",
        "plt.title('Occurrences des Classes dans les Annotations')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DcDuHPOEXMd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzXTu14Flmku"
      },
      "outputs": [],
      "source": [
        "!pip install gtts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts playsound opencv-python-headless\n"
      ],
      "metadata": {
        "id": "xzpbo4hvLTtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "from gtts import gTTS\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Charger le modèle YOLOv5 pré-entraîné\n",
        "model = YOLO ('/content/best_model (5).pt')\n",
        "\n",
        "# Méthode pour obtenir une couleur unique pour chaque classe basée sur l'indice de la classe\n",
        "def get_color(index):\n",
        "    colors = [\n",
        "        (0, 255, 0),   # Vert\n",
        "        (255, 0, 0),   # Rouge\n",
        "        (0, 0, 255),   # Bleu\n",
        "        (255, 255, 0), # Jaune\n",
        "        (0, 255, 255), # Cyan\n",
        "        (255, 0, 255)  # Magenta\n",
        "    ]\n",
        "    return colors[index % len(colors)]\n",
        "\n",
        "# Chemin vers le répertoire des images de test\n",
        "test_images_dir = '/content/road_sign_detection/test/images'\n",
        "test_images = [os.path.join(test_images_dir, img) for img in os.listdir(test_images_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
        "\n",
        "# Générer une description des résultats\n",
        "def generate_description(results):\n",
        "    descriptions = []\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            class_id = int(box.cls)\n",
        "            score = float(box.conf)\n",
        "            descriptions.append(f\"Detected {model.names[class_id]} with confidence {score:.2f}\")\n",
        "    return \" \".join(descriptions)\n",
        "\n",
        "# Convertir le texte en parole\n",
        "def text_to_speech(text):\n",
        "    if text:\n",
        "        tts = gTTS(text=text, lang='en')\n",
        "        tts.save(\"output.mp3\")\n",
        "        ipd.display(ipd.Audio(\"output.mp3\"))\n",
        "    else:\n",
        "        print(\"No detections to speak about.\")\n",
        "\n",
        "# Parcourir les images de test et afficher les résultats\n",
        "for img_path in test_images:\n",
        "    results = model(img_path)\n",
        "    description = generate_description(results)\n",
        "    print(description)\n",
        "    text_to_speech(description)\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    for result in results:\n",
        "        for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            class_id = int(cls)\n",
        "            class_name = model.names[class_id]\n",
        "            confidence = float(conf) * 100  # Convertir en pourcentage\n",
        "            label = f\"{class_name}: {confidence:.1f}%\"\n",
        "            color = get_color(class_id)  # Obtenir la couleur basée sur l'indice de la classe\n",
        "\n",
        "            # Dessiner les boîtes de délimitation et les labels\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.8, color, 1, cv2.LINE_AA)  # Utilisation d'une police plus lisible\n",
        "\n",
        "    # Augmenter la taille de l'image de sortie\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.title(description)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VgtBWLy9IW_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}